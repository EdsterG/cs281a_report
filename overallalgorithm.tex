\section{Solving Task and Motion Planning Problems}
\subsection{SFRCRA-14}
Solving TAMP problems requires evaluation of
possible courses of action comprised of different combinations of
instantiated action operators. This is particularly challenging
because the set of possible action instantiations (and thus the
branching factor of the underlying search problem) is infinite.
We give a brief overview of SFRCRA-14, a recent approach to TAMP, and
refer the interested reader to the cited paper for further details.

SFRCRA-14 solves TAMP problems by: incrementally
searching for a high-level plan that solves the logical abstraction
of the given TAMP problem; determining a prefix of the plan that has a
motion planning feasible refinement; updating the high-level
abstraction to reflect the reason for infeasibility; and searching for
a new plan suffix from the failure step onwards. This search process
addresses the fundamental TAMP problem: high-level
logical descriptions are lossy abstractions of the true environment
dynamics and thus may not include sufficient information to
determine the true applicability of a sequence of actions.

In general, including geometric properties in the logic-based formulation leads to an
increase in the number of objects representing distinct poses and/or trajectories. For
instance, expressing the fact that a trajectory for grasping \emph{can$_1$} is obstructed by
\emph{can$_3$} from the current pose of the robot would require setting a fluent of the
form \emph{obstructs(can$_3$, pose$_{17877}$, trajectory$_{3219}$, can$_1$)} to true in
the description of the high-level state. In turn, this would require adding
\emph{pose$_{17877}$} and \emph{trajectory$_{3219}$} into the set of objects if they were
not already included. Unfortunately, the size of the abstracted, logic-based state space
grows exponentially with the number of objects, and such an approach quickly leads to
unsolvable task planning problems.

SFRCRA-14 addresses this challenge by abstracting the continuous
action arguments, such as robot grasping poses and trajectories, into
a \emph{bounded} set of symbolic references to potential values. A
\emph{high-level}, or \emph{symbolic}, plan refers to the fixed task
sequence returned by a task planner and comprised of these symbolic
references. An \emph{interface layer} conducts plan refinement,
searching for instantiations of continuous values for symbolic
references while ensuring action feasibility.  The resulting process
is able to utilize off-the-shelf task and motion planners while
carrying out the necessary exchange of information in a scalable
manner.

\subsection{Plan Refinement Graph}
Unpublished work recently submitted for review to ICRA 2016 builds on SFRCRA-14 develops a complete algorithm
that maintains a \emph{plan refinement graph} (PRGraph). Every node $u$ in the PRGraph
represents a high-level plan $\pi_u$ and the current state of the search
for a refinement. An edge $(u,v)$ in the PRGraph
represents a ``correction'' of $\pi_u$ for a specific instantiation of
the symbolic references in $\pi_u$. Let $\pi_{u,k}$ be the plan prefix of
$\pi_u$ consisting of the first $k$ actions. Formally, each edge
$e=(u,v)$ is labeled with a tuple $\langle \sigma, k, \varphi \rangle$.
$\sigma$ denotes an instantiation of references for a prefix $\pi_{u,k}$ of
$\pi_u$ such that feasible motion plans have been found for all
previous actions $\pi_{u,k-1}$. $\varphi$ denotes a conjunctive formula
consisting of fluent literals
that were required in the preconditions of the $k^{th}$ action in
$\pi_u$ but were not true in the state obtained upon
application of $\pi_{u,k-1}$ with the instantiation $\sigma_k$.  The
plan in node $v$ (if any) retains the prefix $\pi_{u,k-1}$ and solves
the new high-level problem which incorporates the discovered facts $\varphi_{u,v}$
in the $k^{th}$ state.

The overall search algorithm then interleaves the search for feasible
refinements of each high-level plan with the addition into the
PRGraph of new edges and plan nodes using the semantics described above.

\subsection{Previous Approach for Learning to Search the Graph}
TODO: clean this up a lot

Chitnis et al.~\cite{chitnis2015mlpc} explain that the high level has a two-tiered
decision to make: which node in the plan refinement graph to
visit next, and whether to attempt to refine this node or generate failure information
from it. They develop heuristics that are trained to estimate
the difficulty associated with refining a plan, to make this decision intelligently.

To select between the potential refinement options, the authors learn decision tree regressors
to answer the following questions about a single node $n$ containing plan $p$: 1) how many iterations of randomized refinement
would be needed to achieve a valid refinement for $p$ ($\infty$ if $p$ has no valid refinement); 2)
if we quickly generate a child node $n'$ under $n$ by discovering geometric facts and producing a new plan $p'$,
how many iterations would be needed to refine $p'$? They approach this by learning an estimate of the
number of iterations needed to refine \emph{each} action. To obtain an estimate for a full plan, they
sum this number across all of the plan's actions. This implicitly assumes that dependencies in plans are, in some way, local;
it only makes sense if a plan can be split into subportions with independent refinements.

To train the regressors, we fix pre-trained policies for plan refinement and construct datasets
for supervised learning as follows. For the first regressor, we run refinement on the root
node of the graph over 500 random environments sampled from $\Prob$, and measure the feature vector and number of iterations
until valid refinement (arbitrarily large if no valid refinement exists).
For the second regressor, we do the same, but on a single child node spawned from the root node.
We then fit standard decision tree regressors to our data.

At test time, we make the decision $(v, b)$ as follows. We select $v$ according to a softmin (with decreasing temperature) over the values
predicted by the first regressor. Then, we select $b$ using a softmin comparison between the two regressors'
predicted values for $v$. If refining a child node would reduce the number of steps to a valid refinement,
we bias toward generating a child node.